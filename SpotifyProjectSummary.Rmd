---
title: "Econometrics Research Project"
author: "Karl Konz"
date: "December 15, 2016"
output: pdf_document
---

# Introduction

Spotify has become one of the largest data sources for commercial music recordings since it was developed 10 years ago, with currently over 30 million songs available for digital streaming and 100 million active users. While the transition from physical music recordings to digital solutions has been uneasy to say the least, there has been research that suggests the transition has not affected the music industry positively or negatively in any significant way. One clear benefit from this transition is that consumers now leave a digital foot print of their behavior for which a variety of interesting hypothesizes can be tested, especially when integrated with other data sources. For example, do consumers tend to listen to classical music more when they are at work than in their free time? In a complete factorial designed experiment, do athletes who listen to rap music while working out achieve higher performance measures than when they listen to folk music? This research project aims to predict the total number of streams a given track has based on audio features of the track and metrics of the tracks chart performance such as its maximum and minimum positions and how many days the track was on the top 200 charts.  
For this project we will conduct a predictive analysis using data from the Spotify charts site and data endpoints from their API to attempt to predict the number of streams a given track had while it was on the charts. From the data available, we will create a few derived features and run a random forest model to determine the overall importance of each feature. With the variables that have particularly high influence on the total number of streams an OLS regression will be create with the total number of streams while on the charts as the dependent variable. The reason to run the additional OLS model is that it will be easier to interpret the OLS model than the random forest model. The idea of the project is that the total number of streams that an artist has had can be directly translated into a gross monetary value by multiplying the number of streams by the streaming pay rate. 

# Literature review

The paper _Streaming Reaches Flood Stage: Does Spotify Stimulate or Depress music sales?_ explores current climate of the recorded music industry and if the streaming medium has monetarily influenced the music industry compared to traditional track sales. Overall the conclusion of the article suggests that streaming music has not monetarily changed the industry for better or worse. This project complements the research done in this paper by estimating the monetary returns for artists who have tracks on the daily top 200 charts. While this project is doesn't take traditional physical music recording sales into account, it does give an idea of the returns artists are having when their music hits the top charts. Based on the results from the paper referenced here, one can reasonably skip comparing digital to physical sales unless one wishes to reproduce those findings.

The paper _Using Data in Decision-Making: Analysis from the Music Industry_ sheds light on the changes in the music industry in recent years as well as making suggestions for how and why it is important to utilize data driven decisions in the modern music industry. It references illegal piracy has diminished as music is being consumed through streaming and that there is a lot more information about the consumer that is captured when the consumption is done via digital means. This paper also nicely complements the aims of this project and further aspirations discussed in the introduction and conclusion of this project in terms of integrating multiple data sources to draw new insights from.


The focus of the paper _Combining Spotify and Twitter Data for Generating a Recent and Public Dataset for Music Recommendations_ aims to create a recommendation system for predicting like tracks through Spotify and Twitter data. While this papers focus is on recommending systems, this paper is relevant to my project because it utilizes data mining to gather data through twitter to determine how many times a track has been played. This is paper broaches one of the ultimate goals of this project in general which extends beyond the work displayed here. Unfortunately, the recommender algorithms built into Spotify and what was created in the paper referenced above give more weight to artists and tracks that already have high numbers of streams. The final step of this research would be to not only predict within a reasonable estimate the returns for a given artist, but to create recommenders that attempt to level the playing field so to speak for new artists. One way to do this would be to incorporate the "hipster" tag when pulling in artists via API calls. The "hipster" tag returns only results that have artists popularity of 10 and below. Another aim to extend this project once artist genres can be taken into account to also perform hypothesis tests to see if there are certain genres of music that have seasonal effects or are more popular at certain times of the year. Given the time frame of the project for this class, unfortunately the researcher for this project did not have enough time to clean the data in such a way that genres could be clustered into sub groups as opposed to simply treating all of the tracks on the top 200 daily streamed charts as "pop" music. One anecdotal example of a genre that would likely have increased popularity based on seasonal timing would be Christmas music which undoubtedly has higher numbers of streams towards the end of December as opposed to say in May. An application of such modeling techniques would be for a entertainment management company to create a strategic plan for an artist to release certain types of tracks at various times of the year.

# Data

The most time consuming portion of this project was in collecting and cleaning the data. An end user has the ability to view the number of streams for a given track if they are using the GUI, but the API does not allow such data to be pulled. The charts site does however allow an end user to download a given chart via URL downloads which is in the form of a csv. For this project the data from the daily top 200 global charts are used. With this data set a date variable needs to be manually added. In this data the Track ID can be used to query the Spotify API to pull additional details about the tracks can be obtained. The data from the API is in JSON format and will need to be flattened into a flat file for modeling in R utilizing the httr package to make the JSON calls. From the API this project utilizes the Audio Features and Track endpoints. The API allows for 100 calls to be made at a time for the Audio Features and 50 at a time for the track API. The full script for downloading the data from the URLS and the API can be found in the GitHub repository HERE. There are 18 audio features that we will include in the modeling process from, the track popularity metric will be pulled from the Track endpoint, and the total number of streams for each track is obtained from the data downloaded from the charts site. Upon downloading the full data set it should be noted that there are a number of days where the charts are unavailable as well as a small number of tracks where either the track audio features are missing or the track is simply not available through Spotify anymore. To download the data for this project, run the DownloadTop200.R, ReadData.R, and the APIpull.R scripts. The directories will need to changed and the SpotifyData folder will need to be empty so that the downloaded chart data can be downloaded there and rolled up into one dataset.  








# Analysis




The first step is to load the data into memory using R and to set the seed so that the results will be reproducible. Next, the data will be partitioned into a training set that will be used to create the model and a testing set that will be used to test the accuracy of the model. In this project, the caret package will be used using the createDataPartition() function and will be split into 60% training data and 40% testing data. 




```{r, message=F, warning=F}



# Set the seed to reproduce the same results as shown
# in this script



set.seed(654321)



# Read Data 

SpotifyData <- read.csv("C:/Users/karlk/Desktop/SpotifyData.csv", header = TRUE)

# Load the caret package

library(caret)
library(randomForest)

# Use the createDataPartition function to split the data into training and testing data sets
# 60% of the data will be used for training the data and 40% will be used to test the model

InTrain <- createDataPartition(y=log(SpotifyData$SumChartStreams), p = .60, list=FALSE)

training <- SpotifyData[InTrain, ]
testing <- SpotifyData[-InTrain, ]
```

The next step will be to start modeling the data. It will be advantageous to do some sort of testing for feature importance. Instead of using AIC or BIC stepwise selections for determining what variables to include in the model random forest will be used to make those decisions. With this ensemble learning model, repeated cross validation will be used by using 5 folds in the data and will be repeated 5 times to avoid over fitting the importance scores.

```{r}

# Setup the repeated cross validation for the random forest model that will be used to 
# find the important features to use in the model

control <- trainControl(method="repeatedcv", number=5, repeats=5)

fitImportance <- train(log(SumChartStreams) ~ liveness + danceability + 
                         energy + key + loudness + mode + 
                         speechiness + acousticness + instrumentalness +
                         valence + tempo + log(as.numeric(duration_ms)) + 
                         DateIndex + time_signature +
                         MaxPosition + minPosition +  log(CountOnCharts) + 
                         DateIndex +TrackPopularity, data=training
                       , method="rf", importance = TRUE,
                       preProcess="scale", 
                       trControl=control)






```

The importance ratings are determined by calculating the MSE on out of bag data for each of the trees, then MSE is computed after permuting a variable. The difference from the 2 accuracies are then averaged and normalized by the standard deviation and scaled to be between 0 and 100. Below we can see that the log count of days on the charts, the minimum position, date index, max position, and track popularity all have the highest importance scores for predicting the sum of streams while the track was on the charts.

```{r}

# Create an importance object from the random forest model
importance <- varImp(fitImportance)

# Display the importance scores
print(importance)

```

One caveat to take into consideration is that random forest models do not account for multicollinearity for very well if variables have similar effects on the dependent variable. Below we will create a correlation matrix between the important predictors to examine if there are any variables that could be potentially problematic to include in the final OLS model.

```{r, message=F, warning=F}

# Create a matrix of the important independent variables selected from the random forest.

CorMatrix <- SpotifyData[ , c("CountOnCharts", 
                               "minPosition",
                               "DateIndex", 
                              "MaxPosition",
                              "TrackPopularity")]

# Load the corrplot package to create a correlation plot from the data frame that was created above
library(corrplot)

M <- cor(CorMatrix)

# Pass number to the method parameter
corrplot(M, method="number")

```

The illustration above shows that minPosition and TrackPopularity have moderate correlations with other independent variables. Utilizing Occam's razor theory, including those variables will likely obfuscate inference and since they likely won't increase the goodness of fit with the model, they will be omitted so that the final model will be more interpretable.

```{r}




# Fit an OLS model with the important features from the fitImportance random forest model

fit <- lm(log(SumChartStreams) ~ 
            MaxPosition +  
            DateIndex + 
            log(CountOnCharts),
          data = training)




# View the summary statistics from the OLS model created above


summary(fit)



```


```{r}

anova(fit)

```
Next, to make sure that there aren't any lurking multicollinearity issues a variable inflation test will be performed. If any of the variables have a vif score of 10 or above they should be removed from the model.

```{r, message=F, warning=F}

# Calculate the variable inflation factors to determine if there is any additional lurking multicollinearity issues

library(car)
vif(fit)
```

The vif scores look great so the next step will be examining the residuals to see if model is homoscedastic and normally distributed. Based on the residual histogram and the normal q-q plot, it can be observed that there is a slight deviation on the upper tail, but is for the most part normally distributed. The residual index plot shows that the data is more or less homoscedastic.

```{r}

par(mfrow = c(3,2))

plot(fit)

plot(fit$residuals)

hist(fit$residuals, col = "red")


```

Finally to get a an idea of the accuracy of the model, the predicted estimates for log(SumOnCharts) are plotted against the actual observed values for both the training and testing data sets below. If testing predicted values are not very close to the observed values we can deduce that the model is overfitting the data. 

```{r, fig.height=5}

par(mfrow = c(2,1))

testing$pred <- predict(fit, testing)
training$pred <- predict(fit, training)

#Compare the predicted versus observed values in the testing and training data

plot(testing$pred, log(testing$SumChartStreams))
plot(training$pred, log(training$SumChartStreams))
```

# Conclusion

The model that was built on the training data appears to apply very well to the testing data, which was not included in the model fitting process. For every position dropped there is roughly a .8761 % decrease in the sum of streams in the model. The lower the number of position, the more streams we would expect. For example, a track at position #1 will have the most number of streams for that day, so the negative coefficient for that term makes sense. For every unit increase in the log number of days on the chart, one can estimate that the number of streams would increase by roughly 1.14%. For every day closer to the current date, based on an index starting on 5/1/2015, one would expect a .1066% increase in the number total number of streams. With this data we can estimate the payout amount for a given track. Spotify advertises that payout for one stream is between $0.006 and $0.0084, the mean payout is then $.0072 per stream. Based on the observed data that was downloaded, the average total sum of streams for the duration of days on the top charts is about 35,028,215 streams. Thus we can deduce the average track that makes it to the Spotify charts can yield the artists a payout of roughly $252,203. This amount validates the claims of the referenced literature that transition to digital streaming can still be profitable for recording artists, though this amount is admittedly a gross number which doesn't any overhead into account. There is an enormous amount of data that is created from users using services like Spotify which was discussed in the literature review of this paper. It is also worth noting that the user has incentive to use these services based on the massive amounts of music available for streaming, which overcomes availability constraints of traditional physical recording consumption. The main purpose of this project was to test the hypothesis that one can predict the number of streams a given artist has based on the timing of it being on the charts and to see if there is any statistically significant relationships between the audio features of a given track. Due to time constraints, the genres of the artists were not taken into consideration, this separation of comparing like data to like data would likely have yielded stronger relationships with the audio features of a given genre. The next step of this project will be to create models based on the different sub genres of music on the charts. It may also be advantageous to include social media references to given artists and consider artists touring behaviors into account to get a better idea for predicting monetary gains. Ultimately it would be great to create new modeling techniques that allow artists to get more exposure by strategically planning when to release certain types of tracks. These insights could also be combined with other data sources to make better data driven decisions for touring, video release, and interviews to maximize returns on investment. 

##Literature review sources:

http://eds.a.ebscohost.com.libproxy.stcloudstate.edu/eds/detail/detail?vid=1&sid=3f9505b2-5377-449c-9738-187362b5f820%40sessionmgr4008&hid=4210&bdata=JnNpdGU9ZWRzLWxpdmUmc2NvcGU9c2l0ZQ%3d%3d#AN=edsgcl.368584*64&db=edsgao 

_Note the link above does not appear to be working, I have uploaded this paper to D2l instead._

https://ec.europa.eu/jrc/sites/jrcsh/files/JRC96951.pdf

http://ceur-ws.org/Vol-1313/paper_7.pdf



